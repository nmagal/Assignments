{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zryL8YFkOLC"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_R-kRyUuLuj"
      },
      "source": [
        "Installs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwauV9pb3Ilw"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/ahmetumutdurmus/awd-lstm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fra-b76uQUB"
      },
      "outputs": [],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjP14btJDqA5"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/PetrochukM/PyTorch-NLP.git\n",
        "%cd PyTorch-NLP\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs2Jk4oCycgL"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW42bzFB_ccd"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvwFon3uUXr"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DHAB_E8g6u8",
        "outputId": "d813a08b-6f29-48e7-904d-2796dfcdb511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/awd-lstm\n"
          ]
        }
      ],
      "source": [
        "cd awd-lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWCZrsC2kKAw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pdb \n",
        "import pandas as pd\n",
        "from psutil import virtual_memory\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "import shutil\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "from Levenshtein import distance\n",
        "import torchaudio\n",
        "from google.colab import drive\n",
        "from collections import OrderedDict\n",
        "from torchnlp.nn import WeightDropLSTM\n",
        "from ntasgd import NTASGD\n",
        "from os import listdir\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_hxAQyKjSqd",
        "outputId": "4cf41673-710e-4b8d-dca4-e9d4cb78a274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB6kjT_4lgVP"
      },
      "outputs": [],
      "source": [
        "class SetUpColab():\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "    \n",
        "  #Determines how much ram the runtime has\n",
        "  @staticmethod\n",
        "  def runtime_info():\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "    if ram_gb < 20:\n",
        "      print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "      print('You are using a high-RAM runtime!')\n",
        "    !nvidia-smi\n",
        "    \n",
        "  @staticmethod\n",
        "  def mount_google_drive():\n",
        "    drive._mount('/content/gdrive')\n",
        "  \n",
        "  #Sets up environement for use with kaggle api\n",
        "  @staticmethod\n",
        "  def set_up_kaggle():\n",
        "    !pip uninstall -y kaggle\n",
        "    !pip install --upgrade pip\n",
        "    !pip install kaggle==1.5.6\n",
        "    !mkdir .kaggle\n",
        "\n",
        "    token = {\"username\":\"nicholasmagal\",\"key\":\"9bf671834d75b58fac2b037da15f4cf0\"}\n",
        "    with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "      json.dump(token, file)\n",
        "    \n",
        "    for i in range(2):\n",
        "      !chmod 600 /content/.kaggle/kaggle.json\n",
        "      !cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "      !kaggle config set -n path -v /content\n",
        "\n",
        "  @staticmethod\n",
        "  def setup_wandb():\n",
        "    wandb.login()\n",
        "\n",
        "  #Calls above methods to do a complete Collab setup, ready to run ml models :D Note may want to change this per competition\n",
        "  @staticmethod\n",
        "  def complete_set_up():\n",
        "    SetUpColab.runtime_info()\n",
        "    SetUpColab.mount_google_drive()\n",
        "    SetUpColab.set_up_kaggle()\n",
        "    SetUpColab.setup_wandb()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FA6_28PltGs"
      },
      "outputs": [],
      "source": [
        "SetUpColab.complete_set_up()\n",
        "!kaggle competitions download -c 11785-fall2021-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuoMuq-cmzVM"
      },
      "outputs": [],
      "source": [
        "%cd . \n",
        "!unzip /content/competitions/11785-fall2021-hw3p2/11785-fall2021-hw3p2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXjk1PXa9ukr"
      },
      "source": [
        "#Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDU9JYgiKeGU"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(dataset, shuffle, batch_size = 128, num_workers = 1, pin_memory = True, collate_fn = None):\n",
        "  data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                                  batch_size = batch_size,\n",
        "                                                  shuffle = shuffle,\n",
        "                                                  num_workers = num_workers,\n",
        "                                                  pin_memory = pin_memory,\n",
        "                                                  collate_fn = collate_fn)\n",
        "  return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtgpuILn9uU5"
      },
      "outputs": [],
      "source": [
        "# Define dataset class\n",
        "class UtteranceDatset(Dataset):\n",
        "  # load the dataset\n",
        "  def __init__(self, x_path, y_path):\n",
        "    self.X = np.load(x_path, allow_pickle=True)\n",
        "    self.Y = np.load(y_path, allow_pickle=True)\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    y = torch.LongTensor(self.Y[index])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    features, labels = zip(*batch)\n",
        "    features_length = [len(x) for x in features]\n",
        "    labels_length =[len(y) for y in labels]\n",
        "\n",
        "    features = torch.nn.utils.rnn.pad_sequence(features, batch_first = True)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first = True)\n",
        "\n",
        "    #Features returned as B X L X Frequency Bins\n",
        "    #Labels returned as B X L\n",
        "    return(features, labels, features_length, labels_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvdoq0hu2LN7"
      },
      "outputs": [],
      "source": [
        "# Define dataset class\n",
        "class UtteranceDatset_Test(Dataset):\n",
        "  # load the dataset\n",
        "  def __init__(self, x_path):\n",
        "    self.X = np.load(x_path, allow_pickle=True)\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    return x\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    features_length = [len(x) for x in batch]\n",
        "    features = torch.nn.utils.rnn.pad_sequence(batch, batch_first = True)\n",
        "\n",
        "    #Features returned as B X L X Frequency Bins\n",
        "    #Labels returned as B X L\n",
        "    return(features, features_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEIrv0RoP0n9"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHqLIrOnBThg"
      },
      "outputs": [],
      "source": [
        "class GRUModelConv(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(GRUModelConv, self).__init__()\n",
        "    self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = True, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Dropout(p=dropout))\n",
        "    self.biGRU_0 = nn.GRU(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.biGRU_1 = nn.GRU(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "\n",
        "    x = x.permute(0, 2, 1)\n",
        "    #x = self.transforms(x)\n",
        "    x = self.convolution_embedding(x)\n",
        "\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biGRU_0(x)\n",
        "    x, state = self.biGRU_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qHyqLIe62lF"
      },
      "outputs": [],
      "source": [
        "class LSTMModelConv(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelConv, self).__init__()\n",
        "    #frequecy masking, timemasking, catuion noise, augmentations \n",
        "    #self.transforms = nn.Sequential(torchaudio.transforms.FrequencyMasking(freq_mask_param=15),torchaudio.transforms.TimeMasking(time_mask_param=15))\n",
        "    self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Dropout(p=dropout))\n",
        "    self.biLSTM_0 = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.biLSTM_1 = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "    \n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    #x = self.transforms(x)\n",
        "    x = self.convolution_embedding(x)\n",
        "\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cz1ozEHLe6m"
      },
      "outputs": [],
      "source": [
        "class LSTMModelConvModifiedBigNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelConvModifiedBigNetwork, self).__init__()\n",
        "    #frequecy masking, timemasking, catuion noise, augmentations \n",
        "    #self.transforms = nn.Sequential(torchaudio.transforms.FrequencyMasking(freq_mask_param=15),torchaudio.transforms.TimeMasking(time_mask_param=15))\n",
        "    self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.LeakyReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.LeakyReLU(), nn.BatchNorm1d(256),nn.Dropout(p=.3))\n",
        "    self.biLSTM_0 = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=False, dropout = dropout, batch_first = True)\n",
        "    self.biLSTM_1 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.linear_layers = nn.Sequential(nn.LeakyReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.LeakyReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "\n",
        "    x = x.permute(0, 2, 1)\n",
        "    #x = self.transforms(x)\n",
        "    #Transforms\n",
        "\n",
        "    x = self.convolution_embedding(x)\n",
        "\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehn9WlbIEEwp"
      },
      "outputs": [],
      "source": [
        "class LSTMModelVanilla(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelVanilla, self).__init__()\n",
        "\n",
        "    self.biLSTM_0 = nn.LSTM(input_size=40, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.biLSTM_1 = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_57aR0lrm8wH"
      },
      "outputs": [],
      "source": [
        "class LSTMModelConvTransforms(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelConvTransforms, self).__init__()\n",
        "    #frequecy masking, timemasking, catuion noise, augmentations \n",
        "    self.transforms = nn.Sequential(torchaudio.transforms.FrequencyMasking(freq_mask_param=15),torchaudio.transforms.TimeMasking(time_mask_param=100))\n",
        "    self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Dropout(p=.4))\n",
        "    self.biLSTM_0 = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.biLSTM_1 = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = self.transforms(x)\n",
        "    #Transforms\n",
        "    x = self.convolution_embedding(x)\n",
        "\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAaknHLbN0jC"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, layers_per_lstm,dropout):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = True, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Dropout(p=dropout))\n",
        "\n",
        "        self.blstm_0 = nn.LSTM(input_size = 256, hidden_size = hidden_dim, num_layers = layers_per_lstm, bidirectional = True, batch_first = True)\n",
        "        self.blstm_1 = nn.LSTM(input_size = hidden_dim*2, hidden_size = hidden_dim, num_layers=layers_per_lstm,bidirectional = True, batch_first = True)\n",
        "        #self.blstm_2 = nn.LSTM(input_size = hidden_dim*2, hidden_size = hidden_dim, num_layers=layers_per_lstm,bidirectional = True, batch_first = True)\n",
        "        #self.blstm_3 = nn.LSTM(input_size = hidden_dim*2, hidden_size = hidden_dim, num_layers=layers_per_lstm,bidirectional = True, batch_first = True)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "        self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(hidden_dim*2, 128, bias=False), nn.ReLU(), torch.nn.Dropout(p = dropout), nn.Linear(128,42))\n",
        "\n",
        "    def forward(self, x, lengths_data):\n",
        "      # x dimen =  B X L X C\n",
        "      # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "      x = x.permute(0, 2, 1)\n",
        "      x = self.convolution_embedding(x)\n",
        "\n",
        "      #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "      x = x.permute(0,2,1)\n",
        "      x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "      x, state = self.blstm_0(x)     \n",
        "\n",
        "      #First level of pLSTM\n",
        "      x, batch_lengths = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "      x = self.dropout(x)\n",
        "      if x.shape[1]%2 !=0:\n",
        "        x = x[:, 0:-1, :]\n",
        "      batch_lengths = [lens//2 for lens in batch_lengths]\n",
        "      x = torch.reshape(x, (x.shape[0], int(x.shape[1]/2), 2, x.shape[2]))\n",
        "      x = torch.mean(x, dim = 2)\n",
        "      x = torch.nn.utils.rnn.pack_padded_sequence(x, batch_lengths, enforce_sorted=False, batch_first=True)\n",
        "      x, state = self.blstm_1(x)\n",
        "\n",
        "      # #Second level of pLSTM\n",
        "      # x, batch_lengths = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
        "      # x = self.dropout(x)\n",
        "      # if x.shape[1]%2 !=0:\n",
        "      #   x = x[:, 0:-1, :] \n",
        "      # batch_lengths = [lens//2 for lens in batch_lengths]\n",
        "      # x = torch.reshape(x, (x.shape[0], int(x.shape[1]/2), 2, x.shape[2]))\n",
        "      # x = torch.mean(x, dim = 2)\n",
        "      # x = torch.nn.utils.rnn.pack_padded_sequence(x, batch_lengths, enforce_sorted=False, batch_first=True)\n",
        "      # x, state = self.blstm_2(x)\n",
        "\n",
        "      # #Third level of pLSTM - This causes NAN as output for training loss\n",
        "      # x, batch_lengths = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
        "      # x = self.dropout(x)\n",
        "      # if x.shape[1]%2 !=0:\n",
        "      #   x = x[:, 0:-1, :] \n",
        "      # batch_lengths = [lens//2 for lens in batch_lengths]\n",
        "      # x = torch.reshape(x, (x.shape[0], int(x.shape[1]/2), 2, x.shape[2]))\n",
        "      # x = torch.mean(x, dim = 2)\n",
        "      # x = torch.nn.utils.rnn.pack_padded_sequence(x, batch_lengths, enforce_sorted=False, batch_first=True)\n",
        "      # x, state = self.blstm_3(x)\n",
        "      \n",
        "      #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "      x, batch_lengths = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
        "      x = self.linear_layers(x)\n",
        "      return(x,batch_lengths)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptt3zSwtPeue"
      },
      "outputs": [],
      "source": [
        "class LSTMModelConvDropConnect(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelConvDropConnect, self).__init__()\n",
        "    #frequecy masking, timemasking, catuion noise, augmentations \n",
        "    #self.transforms = nn.Sequential(torchaudio.transforms.FrequencyMasking(freq_mask_param=15),torchaudio.transforms.TimeMasking(time_mask_param=15))\n",
        "    self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Dropout(p=dropout))\n",
        "    self.biLSTM_0 = WeightDropLSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True, weight_dropout = dropout)\n",
        "    self.biLSTM_1 = WeightDropLSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True, weight_dropout = dropout)\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "  \n",
        "    x = x.permute(0, 2, 1)\n",
        "    #x = self.transforms(x)\n",
        "    #Transforms\n",
        "    x = self.convolution_embedding(x)\n",
        "\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6O6IpDpUyjY"
      },
      "outputs": [],
      "source": [
        "class LSTMModelConvDropConnect2(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelConvDropConnect2, self).__init__()\n",
        "    #frequecy masking, timemasking, catuion noise, augmentations \n",
        "    #self.transforms = nn.Sequential(torchaudio.transforms.FrequencyMasking(freq_mask_param=15),torchaudio.transforms.TimeMasking(time_mask_param=15))\n",
        "    self.convolution_embedding = nn.Sequential(nn.Conv1d(in_channels = 40, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size=3, bias = False, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),nn.Dropout(p=dropout))\n",
        "    self.biLSTM_0 = WeightDrop(nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True),['weight_hh_l0'], dropout = dropout )\n",
        "    self.biLSTM_1 = WeightDrop(nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True),['weight_hh_l0'], dropout = dropout )\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "  def forward(self, x, lengths_data): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "  \n",
        "    x = x.permute(0, 2, 1)\n",
        "    #x = self.transforms(x)\n",
        "    #Transforms\n",
        "    x = self.convolution_embedding(x)\n",
        "\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhzW2f0Ocbes"
      },
      "outputs": [],
      "source": [
        "class LSTMModelResnet(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_layers, dropout):\n",
        "    super(LSTMModelResnet, self).__init__()\n",
        "    #frequecy masking, timemasking, catuion noise, augmentations \n",
        "    # 10, 10 -- epochs 1-70\n",
        "    #10, 20 -- epchs 70-78\n",
        "    #15, 40 - epcohs 78-?\n",
        "\n",
        "    self.transforms = nn.Sequential(torchaudio.transforms.FrequencyMasking(freq_mask_param=10),torchaudio.transforms.TimeMasking(time_mask_param=20))\n",
        "    self.convolution_embedding = Resnet18(40)\n",
        "    self.biLSTM_0 = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.biLSTM_1 = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers = num_layers, bidirectional=True, dropout = dropout, batch_first = True)\n",
        "    self.linear_layers = nn.Sequential(nn.ReLU(), nn.Linear(2*hidden_size, 128, bias=False), nn.ReLU(), torch.nn.Dropout(), nn.Linear(128,42))\n",
        "    \n",
        "  def forward(self, x, lengths_data, eval_mode = False): \n",
        "    # x dimen =  B X L X C\n",
        "    # CNN dim-Input = (B, C, L), output = (B, C, L)\n",
        "    x = x.permute(0, 2, 1)\n",
        "\n",
        "    if eval_mode == False:\n",
        "      x = self.transforms(x)\n",
        "\n",
        "    x = self.convolution_embedding(x)\n",
        "    #LSTM Input = B x L X C, Output = B X L X Hidden * D \n",
        "    x = x.permute(0,2,1)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths_data, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "    x, state = self.biLSTM_0(x)\n",
        "    x, state = self.biLSTM_1(x)\n",
        "    x, length_1 = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #Linear dim-Iput = * X H, dim-Output = B x L x C\n",
        "    x = self.linear_layers(x)\n",
        "    return(x,length_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WgjElQKTv7M"
      },
      "outputs": [],
      "source": [
        "class ResidualBlockResnet34(nn.Module):\n",
        "  def __init__(self, input_channel_size, keep_dim = True):\n",
        "    super().__init__()\n",
        "\n",
        "    #Depending if we are changing our dimensions, we will have to initlize our parameters differently\n",
        "    if keep_dim == True:  \n",
        "      output_size = input_channel_size\n",
        "      self.shortcut_x = nn.Identity()\n",
        "      self.conv0 = nn.Conv1d(in_channels=input_channel_size, out_channels = output_size, kernel_size=3,stride=1, padding = 'same', bias = False)\n",
        "    \n",
        "    else:\n",
        "      output_size = int(input_channel_size*2)\n",
        "      self.shortcut_x = nn.Sequential(\n",
        "      nn.Conv1d(in_channels=input_channel_size, out_channels= output_size, kernel_size = 3, stride = 1, padding = 'same', bias = False),\n",
        "      nn.BatchNorm1d(output_size))\n",
        "      self.conv0 = nn.Conv1d(in_channels=input_channel_size, out_channels = output_size, kernel_size=3, stride=1, padding = 'same', bias = False)\n",
        "\n",
        "    self.bn_0 = nn.BatchNorm1d(output_size)\n",
        "    self.reLU_0 = nn.ReLU()\n",
        "\n",
        "    self.conv1 = nn.Conv1d(in_channels=output_size, out_channels = output_size, kernel_size = 3, stride = 1, padding='same', bias = False)\n",
        "    self.bn_1 = nn.BatchNorm1d(output_size)\n",
        "    self.reLU_1 = nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut = self.shortcut_x(x)\n",
        "    \n",
        "    out = self.conv0(x)\n",
        "    out = self.bn_0(out)\n",
        "    out = self.reLU_0(out)\n",
        "    \n",
        "    out = self.conv1(out)\n",
        "    out = self.bn_1(out)\n",
        "    out = self.reLU_1(out)\n",
        "\n",
        "    out = out + shortcut\n",
        "\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk83T7kdGEEM"
      },
      "outputs": [],
      "source": [
        "class Resnet18(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=in_channels,out_channels=64,kernel_size=3, stride =1, padding='same',bias=False),\n",
        "        nn.BatchNorm1d(64),\n",
        "        nn.ReLU(), \n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64, keep_dim = False),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        nn.Dropout(p=0.2))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.cnn_layers(x)\n",
        "    return(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_47Vd4k62_J"
      },
      "source": [
        "#Model Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUD2X5r-IrJP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer, loss_function, epoch):\n",
        "  model.train()\n",
        "  running_loss = 0.0 \n",
        "\n",
        "  for features, label, features_length, labels_length in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    features, label = features.to(device), label.to(device)\n",
        "\n",
        "    output, x_lengths = model(features, features_length)\n",
        "    log_output = torch.nn.functional.log_softmax(output, dim =2) \n",
        "    #Permuting from B X L X C to L X B X C\n",
        "    log_output = log_output.permute(1,0,2)\n",
        "\n",
        "    loss = loss_function(log_output, label, x_lengths.tolist(), labels_length)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(),.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    del features\n",
        "    del label\n",
        "    del loss\n",
        "    del log_output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  average_loss = running_loss/len(train_loader.dataset)\n",
        "  print(f\"Epoch:{epoch}, Training Loss:{average_loss}\")\n",
        "  wandb.log({\"Training Average Loss\":average_loss, 'Epoch':epoch })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF5NjXtdmRmf"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, val_loader, loss_function, epoch_index, decoder, batch_size):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_dist = 0.0\n",
        "  predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for features, labels, features_length, labels_length in val_loader: \n",
        "      features, labels = features.to(device), labels.to(device)\n",
        "      \n",
        "      output, length = model(features, features_length, eval_mode = True)\n",
        "      log_output = torch.nn.functional.log_softmax(output, dim =2) \n",
        "      #Permuting from B X L X C to L X B X C\n",
        "      log_output = log_output.permute(1,0,2)\n",
        "\n",
        "      decode_result, decoded_label = decode_full_batch(decoder, log_output, length, labels_length, labels)\n",
        "      running_dist += lev_distance_batch(decode_result, decoded_label) \n",
        "\n",
        "      loss = loss_function(log_output, labels, length.tolist(), labels_length)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      del features\n",
        "      del labels\n",
        "      del loss\n",
        "      del log_output\n",
        "      torch.cuda.empty_cache()\n",
        "      \n",
        "  average_loss = running_loss/len(val_loader.dataset)  \n",
        "  average_dist = running_dist/len(val_loader.dataset)\n",
        "  print(f\"Epoch:{epoch_index}, Validation Loss:{average_loss}, Validation Levenshtein Distance: {average_dist}\")\n",
        "  wandb.log({\"Validation Average Loss\":average_loss, \"Validation Levenshtein Distance\":average_dist })\n",
        "  return(average_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyb5lSKGrtvA"
      },
      "outputs": [],
      "source": [
        "def inference(model, test_loader, decoder, batch_size, save_name):\n",
        "  full_predictions_list = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for features, features_length in test_loader:\n",
        "      features = features.to(device)\n",
        "      output, length = model(features, features_length, eval_mode =True)\n",
        "      log_output = torch.nn.functional.log_softmax(output, dim =2)\n",
        "      #Permuting from B X L X C to L X B X C\n",
        "      log_output = log_output.permute(1,0,2)\n",
        "\n",
        "      decode_result, _ = decode_full_batch(decoder, log_output, length)\n",
        "      full_predictions_list.extend(decode_result)\n",
        "  \n",
        "  df = write_model_predictions(full_predictions_list, save_name)\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpA3D1K3c28A"
      },
      "outputs": [],
      "source": [
        "  def save_model( model, optimizer, path, scheduler, epoch):\n",
        "    path = path + str(epoch) + '.pt'\n",
        "    torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict' : scheduler.state_dict(),\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03Jt3y73zalK"
      },
      "outputs": [],
      "source": [
        "def write_model_predictions(predictions,save_name):\n",
        "  df = pd.DataFrame(predictions)\n",
        "  df.columns = ['label']\n",
        "  df['label'] = df['label']\n",
        "  df.index.name = 'id'\n",
        "  df.to_csv('/content/gdrive/MyDrive/IDL/HW/HW3/'+save_name+ '.csv', index = True)\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwVlXBOInVvj"
      },
      "outputs": [],
      "source": [
        "def deploy_model(model, epoch_total, train_loader, optimizer, loss_function, validation_loader, decoder, batch_size, scheduler, path):\n",
        "  for epoch_index in range(epoch_total):\n",
        "    train_model(model, train_loader, optimizer, loss_function, epoch_index)\n",
        "    validation_average_loss = validate_model(model,validation_loader,loss_function,epoch_index,decoder,batch_size)\n",
        "    scheduler.step(validation_average_loss)\n",
        "    save_model(model, optimizer, path, scheduler, epoch_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylVSDokhATth"
      },
      "outputs": [],
      "source": [
        "def load_model_for_inference(save_path, model):\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8N3ojPiBXdH"
      },
      "outputs": [],
      "source": [
        "def load_model_for_more_training(save_path, model, optimizer, schedular):\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    schedular.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    return model, optimizer, schedular"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am-wuBmNShy9"
      },
      "source": [
        "Below functions are used for ensembeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6YuNyETSW7w"
      },
      "outputs": [],
      "source": [
        "class Ensemble():\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  #saves the probability vectors of each model\n",
        "  def save_distributions(self, model_list, test_loader, save_name, log_interpolation = False):\n",
        "    for index, model in enumerate(model_list):\n",
        "      full_predictions_list = []\n",
        "      batch_lengths = []\n",
        "      model.to(device)\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for features, features_length in test_loader:\n",
        "          features = features.to(device)\n",
        "          output, length = model(features, features_length)\n",
        "\n",
        "          if log_interpolation == False:\n",
        "            probability_vectors = torch.nn.functional.softmax(output, dim = 2)\n",
        "          else:\n",
        "            probability_vectors = torch.nn.functional.log_softmax(output, dim = 2)\n",
        "          #Dim - List of [B X L X Phonemes]\n",
        "          full_predictions_list.append(probability_vectors.detach().cpu().numpy())\n",
        "          #No need to run this more then once \n",
        "          if index == len(model_list) - 1:\n",
        "            batch_lengths.append(features_length)\n",
        "\n",
        "      full_predictions_list = np.asarray(full_predictions_list, dtype=object)\n",
        "      save_name += str(index)\n",
        "\n",
        "      with open(save_name, 'wb') as f:\n",
        "        np.save(f, full_predictions_list)\n",
        "    \n",
        "    batch_lengths = np.asarray(batch_lengths)\n",
        "    with open('/content/gdrive/MyDrive/IDL/HW/HW3/batch_lengths/lengths.npy', 'wb') as f:\n",
        "      np.save(f, batch_lengths)\n",
        "\n",
        "  #perform linear or log interpolation ensembling. \n",
        "  def interpolation(self, prob_directory, log_interpolation = False):\n",
        "    probabilities_dir = os.listdir(prob_directory)\n",
        "    probabilities_paths = [ prob_directory+'/'+file_name for file_name in probabilities_dir]\n",
        "    prob_0 = np.load(probabilities_paths[0],allow_pickle=True)    \n",
        "    aux_probs = [np.load(probabilities_paths[i],allow_pickle=True) for i in range(1, len(probabilities_paths))]\n",
        "\n",
        "    for minibatch_index in range(len(prob_0)):\n",
        "      for aux_prob in aux_probs:\n",
        "        prob_0[minibatch_index] += aux_prob[minibatch_index]\n",
        "      prob_0[minibatch_index] = prob_0[minibatch_index]/(len(aux_probs)+1)\n",
        "      prob_0[minibatch_index] = torch.from_numpy(prob_0[minibatch_index])\n",
        "      if log_interpolation == True:\n",
        "        prob_0[minibatch_index] = torch.nn.functional.softmax(prob_0[minibatch_index], dim = 2)\n",
        "    return(prob_0)\n",
        "  \n",
        "  #Writes out the ensembled probabilities to a df for kaggle submission\n",
        "  def ensemble_inference(self, ensembled_probabilities, batch_lengths, decoder, save_name):\n",
        "    full_predictions_list = []\n",
        "    ensembled_output = zip(ensembled_probabilities,batch_lengths)\n",
        "    for probability_vectors, minibatch_length in ensembled_output:\n",
        "      probability_vectors = probability_vectors.permute(1,0,2)\n",
        "      #L x B X Probilities\n",
        "      minibatch_length = torch.from_numpy(np.asarray(minibatch_length))\n",
        "      decode_results, _ = decode_full_batch(decoder,probability_vectors,minibatch_length)\n",
        "      full_predictions_list.extend(decode_results)\n",
        "    df = write_model_predictions(full_predictions_list, save_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyRzEY2qhl-v"
      },
      "source": [
        "#Decoding Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7_ZJ0QMhemg"
      },
      "outputs": [],
      "source": [
        "N_PHONEMES = 41\n",
        "PHONEME_LIST = [\n",
        "    \" \",\n",
        "    \"SIL\",\n",
        "    \"SPN\",\n",
        "    \"AA\",\n",
        "    \"AE\",\n",
        "    \"AH\",\n",
        "    \"AO\",\n",
        "    \"AW\",\n",
        "    \"AY\",\n",
        "    \"B\",\n",
        "    \"CH\",\n",
        "    \"D\",\n",
        "    \"DH\",\n",
        "    \"EH\",\n",
        "    \"ER\",\n",
        "    \"EY\",\n",
        "    \"F\",\n",
        "    \"G\",\n",
        "    \"H\",\n",
        "    \"IH\",\n",
        "    \"IY\",\n",
        "    \"JH\",\n",
        "    \"K\",\n",
        "    \"L\",\n",
        "    \"M\",\n",
        "    \"N\",\n",
        "    \"NG\",\n",
        "    \"OW\",\n",
        "    \"OY\",\n",
        "    \"P\",\n",
        "    \"R\",\n",
        "    \"S\",\n",
        "    \"SH\",\n",
        "    \"T\",\n",
        "    \"TH\",\n",
        "    \"UH\",\n",
        "    \"UW\",\n",
        "    \"V\",\n",
        "    \"W\",\n",
        "    \"Y\",\n",
        "    \"Z\",\n",
        "    \"ZH\"\n",
        "]\n",
        "\n",
        "PHONEME_MAP = [\n",
        "    \" \",\n",
        "    \".\", #SIL\n",
        "    \"!\", #SPN\n",
        "    \"a\", #AA\n",
        "    \"A\", #AE\n",
        "    \"h\", #AH\n",
        "    \"o\", #AO\n",
        "    \"w\", #AW\n",
        "    \"y\", #AY\n",
        "    \"b\", #B\n",
        "    \"c\", #CH\n",
        "    \"d\", #D\n",
        "    \"D\", #DH\n",
        "    \"e\", #EH\n",
        "    \"r\", #ER\n",
        "    \"E\", #EY\n",
        "    \"f\", #F\n",
        "    \"g\", #G\n",
        "    \"H\", #H\n",
        "    \"i\", #IH \n",
        "    \"I\", #IY\n",
        "    \"j\", #JH\n",
        "    \"k\", #K\n",
        "    \"l\", #L\n",
        "    \"m\", #M\n",
        "    \"n\", #N\n",
        "    \"N\", #NG\n",
        "    \"O\", #OW\n",
        "    \"Y\", #OY\n",
        "    \"p\", #P \n",
        "    \"R\", #R\n",
        "    \"s\", #S\n",
        "    \"S\", #SH\n",
        "    \"t\", #T\n",
        "    \"T\", #TH\n",
        "    \"u\", #UH\n",
        "    \"U\", #UW\n",
        "    \"v\", #V\n",
        "    \"W\", #W\n",
        "    \"?\", #Y\n",
        "    \"z\", #Z\n",
        "    \"Z\" #ZH\n",
        "]\n",
        "\n",
        "#This is used to map from integer to letter\n",
        "int_indexes = np.arange(len(PHONEME_MAP))\n",
        "int_to_phoneme_dict = dict()\n",
        "\n",
        "for index in int_indexes:\n",
        "  int_to_phoneme_dict[index] = PHONEME_MAP[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__MHWI88tYZc"
      },
      "outputs": [],
      "source": [
        "def decode(decoder, output, length, labels):\n",
        "  #output is given as L X B X C, ctc_decoder takes B X L X C\n",
        "  output = output.permute(1,0,2)\n",
        "\n",
        "  #Pass in lenght sequence length \n",
        "  beam_results, beam_scores, timesteps, out_len = decoder.decode(output, length)\n",
        "\n",
        "  #This is for one batch\n",
        "  results = beam_results[0][0][:out_len[0][0]].cpu().numpy()\n",
        "\n",
        "  #ADD A FOR FOR BATCHES\n",
        "  decoded_prediction = [PHONEME_MAP[i] for i in results]\n",
        "  decoded_labels = [PHONEME_MAP[i] for i in labels[0]]\n",
        "\n",
        "  #strip padding rstrip \n",
        "  \n",
        "  #Returns the decoded string\n",
        "  decoded_prediction = ''.join(decoded_prediction)\n",
        "  decoded_labels = ''.join(decoded_labels)\n",
        "  return(decoded_prediction, decoded_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR6PIXX7t6e6"
      },
      "outputs": [],
      "source": [
        "def decode_full_batch(decoder, output, length, labels_length=None, labels=None):\n",
        "  #output is given as L X B X C, ctc_decoder takes B X L X C\n",
        "  output = output.permute(1,0,2)\n",
        "\n",
        "  #Pass in lenght sequence length \n",
        "  beam_results, beam_scores, timesteps, out_len = decoder.decode(output, length)\n",
        "\n",
        "  decoded_prediction_list = []\n",
        "  decoded_labels_list = []\n",
        "\n",
        "  #print(\"beam size is\", beam_results.shape)\n",
        "  batch_size = beam_results.shape[0]\n",
        "\n",
        "  for b in range(batch_size):\n",
        "    \n",
        "    results = beam_results[b][0][:out_len[b][0]].cpu().numpy()\n",
        "    decoded_prediction = [PHONEME_MAP[i] for i in results]\n",
        "    decoded_prediction = ''.join(decoded_prediction)\n",
        "    decoded_prediction_list.append(decoded_prediction)\n",
        "\n",
        "    if labels != None:\n",
        "      decoded_labels = [PHONEME_MAP[i] for i in labels[b, :labels_length[b]]]\n",
        "      decoded_labels = ''.join(decoded_labels)\n",
        "      decoded_labels_list.append(decoded_labels)\n",
        "    \n",
        "#Check that we actually need to decode labels\n",
        "  return(decoded_prediction_list, decoded_labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Db5i02DlKW"
      },
      "outputs": [],
      "source": [
        "def lev_distance_batch(list_of_predictions, list_of_labels):\n",
        "  dist = 0\n",
        "  for index, word in enumerate(list_of_predictions):\n",
        "    dist += distance(word,list_of_labels[index])\n",
        "  return(dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xothEpfiGvRu"
      },
      "source": [
        "#Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrSVdqf5DPLr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs' : 30,\n",
        "    'lr' : .0006,\n",
        "    'optimizer' : 'adam',\n",
        "    'batch_size' : 48,\n",
        "    'schedular' : 'ReduceLROnPlateau',\n",
        "    'weight_decay' : 5e-6,\n",
        "    'LSTM_hidden' : 512,\n",
        "    'LSTM_layers' : 2,\n",
        "    'dropout': .25,\n",
        "    'patience' : 5,\n",
        "    'factor' : .2,\n",
        "    'embedding' : 'None',\n",
        "    'beam_width' : 1,\n",
        "    'run_name' : 'run17/continued'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob9Q5v3xlEs-"
      },
      "outputs": [],
      "source": [
        "path_to_toy_train_data = '/content/gdrive/MyDrive/IDL_HW/HW3/Resources/train_data_toy.npy'\n",
        "path_to_toy_train_labels = '/content/gdrive/MyDrive/IDL_HW/HW3/Resources/train_labels_toy.npy'\n",
        "path_to_toy_dev_data = '/content/gdrive/MyDrive/IDL_HW/HW3/Resources/dev_data_toy.npy'\n",
        "path_to_toy_dev_labels = '/content/gdrive/MyDrive/IDL_HW/HW3/Resources/dev_labels_toy.npy'\n",
        "\n",
        "path_to_train_data = '/content/HW3P2_Data/train.npy'\n",
        "path_to_train_labels = '/content/HW3P2_Data/train_labels.npy'\n",
        "path_to_dev_data = '/content/HW3P2_Data/dev.npy'\n",
        "path_to_dev_labels = '/content/HW3P2_Data/dev_labels.npy'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlXe3gfV_EYX",
        "outputId": "c8ac9a0b-a65b-456b-e1cd-8e5140a8db8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is on cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device is on\",device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exITD9JIznjx"
      },
      "outputs": [],
      "source": [
        "decoder = CTCBeamDecoder(\n",
        "      PHONEME_MAP,\n",
        "      beam_width = config['beam_width'],\n",
        "      blank_id = 0,\n",
        "      log_probs_input = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_JyfVzg65mr"
      },
      "outputs": [],
      "source": [
        "rnn = LSTMModelResnet(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "rnn = rnn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3grk3aAEytx"
      },
      "outputs": [],
      "source": [
        "save_path = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/' + config['run_name']\n",
        "\n",
        "train_dataset = UtteranceDatset(path_to_train_data, path_to_train_labels)\n",
        "train_dataloader = create_data_loader(train_dataset, shuffle=True, batch_size=config['batch_size'], collate_fn = UtteranceDatset.collate_fn, num_workers=1)\n",
        "val_dataset = UtteranceDatset(path_to_dev_data,path_to_dev_labels)\n",
        "val_dataloader = create_data_loader(val_dataset, shuffle=True, batch_size=config['batch_size'], collate_fn = UtteranceDatset.collate_fn, num_workers=1)\n",
        "\n",
        "#optimizer = torch.optim.Adam(rnn.parameters(), lr = config['lr'], weight_decay=config['weight_decay'])\n",
        "optimizer = NTASGD(rnn.parameters())\n",
        "ctc_loss = torch.nn.CTCLoss() \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=config['patience'], factor=config['factor'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah1oduxUEgAg"
      },
      "source": [
        "Run the below line if you want to train from a previous save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARi7r1yGEe3u"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run17/continued29.pt'\n",
        "rnn, optimizer, scheduler = load_model_for_more_training(model_path, rnn, optimizer,scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Tp5-nw2YQ8"
      },
      "source": [
        "Else just run this block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zPEqHnZpn9V"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"UtteranceToPhoneme\", config=config)\n",
        "deploy_model(rnn, config['epochs'], train_dataloader, optimizer, ctc_loss, val_dataloader, decoder, config['batch_size'], scheduler, save_path )\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdJiQvLn1ZFk"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SovF4NFFEq4v"
      },
      "source": [
        "Run the below line if you want to run from a save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpiC49hnAsaU"
      },
      "outputs": [],
      "source": [
        "#If we want to load a different model\n",
        "save_path = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run16/continuedd19.pt'\n",
        "\n",
        "#MAKE SURE TO CHANGE MODEL HERE depenending on the run\n",
        "rnn = LSTMModelResnet(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "rnn = rnn.to(device)\n",
        "rnn = load_model_for_inference(save_path, rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akzMrH2k18En"
      },
      "outputs": [],
      "source": [
        "path_to_test_data = '/content/HW3P2_Data/test.npy'\n",
        "test_dataset = UtteranceDatset_Test(path_to_test_data)\n",
        "test_loader = create_data_loader(test_dataset, shuffle=False, batch_size=config['batch_size'], collate_fn = UtteranceDatset_Test.collate_fn, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRvs56y01cQo"
      },
      "outputs": [],
      "source": [
        "df=inference(rnn,test_loader,decoder,config['batch_size'], 'predictions/output90')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEthPJIyc81p"
      },
      "source": [
        "#Ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWpsoU97cJGM"
      },
      "source": [
        "Run the below to ensemble models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag3uteLs-3jh",
        "outputId": "6e273f8e-50ef-49e4-d7d3-4bab6c51b0de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is on cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device is on\",device)\n",
        "decoder = CTCBeamDecoder(\n",
        "      PHONEME_MAP,\n",
        "      beam_width = config['beam_width'],\n",
        "      blank_id = 0,\n",
        "      log_probs_input = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc72EzzmdhwU"
      },
      "outputs": [],
      "source": [
        "model_path_0 = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run6/52.pt'\n",
        "rnn_0 = LSTMModelConv(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "rnn_0 = load_model_for_inference(model_path_0, rnn_0)\n",
        "\n",
        "#model_path_1 = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run8/51.pt'\n",
        "#rnn_1 = pBLSTM(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "#rnn_1 = load_model_for_inference(model_path_1, rnn_1)\n",
        "\n",
        "model_path_2 = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run3/46.pt'\n",
        "rnn_2 = LSTMModelVanilla(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "rnn_2 = load_model_for_inference(model_path_2, rnn_2)\n",
        "\n",
        "model_path_3 = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run12/continued29.pt'\n",
        "rnn_3 = LSTMModelConvDropConnect(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "rnn_3 = load_model_for_inference(model_path_3, rnn_3)\n",
        "\n",
        "model_path_4 = '/content/gdrive/MyDrive/IDL/HW/HW3/model_saves/run13/continued42.pt'\n",
        "rnn_4 = LSTMModelConv(config['LSTM_hidden'], config['LSTM_layers'], config['dropout'])\n",
        "rnn_4 = load_model_for_inference(model_path_4, rnn_4)\n",
        "\n",
        "#DEBUGING TO SEE IF THIS ACTUALLY WORKS\n",
        "#model_list = [rnn_0,rnn_0,rnn_0,rnn_0]\n",
        "model_list = [rnn_0,rnn_2,rnn_3,rnn_4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_g0cR38xuwi"
      },
      "source": [
        "Use this to create probability vectors of each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHJh93SjcIds",
        "outputId": "0d333457-7b4e-498a-b986-5505da69afb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:695: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  self.num_layers, self.dropout, self.training, self.bidirectional)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/gdrive/MyDrive/IDL/HW/HW3/probabilities/probsmodel.npy'\n",
        "ensembler = Ensemble()\n",
        "ensembler.save_distributions(model_list,test_loader,save_path,log_interpolation = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANW9e_7nx0LB"
      },
      "source": [
        "Use this to combine probability vectors into one distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yGfd5R30USw"
      },
      "outputs": [],
      "source": [
        "ensembled_probabilities = ensembler.interpolation('/content/gdrive/MyDrive/IDL/HW/HW3/probabilities',log_interpolation = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idlba83vJHsH"
      },
      "source": [
        "Use this to run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEKJY6UW0Tn0"
      },
      "outputs": [],
      "source": [
        "batch_lengths = np.load('/content/gdrive/MyDrive/IDL/HW/HW3/batch_lengths/lengths.npy', allow_pickle = True)\n",
        "save_path = 'predictions/myoutt51'\n",
        "ensembler.ensemble_inference(ensembled_probabilities, batch_lengths, decoder, save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "FXjk1PXa9ukr",
        "n_47Vd4k62_J",
        "cyRzEY2qhl-v"
      ],
      "name": "UtteranceToPhonemeMapping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}