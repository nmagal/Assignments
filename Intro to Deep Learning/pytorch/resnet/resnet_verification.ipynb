{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_part2_verification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztr8pPQ12J9Y"
      },
      "source": [
        "For this I used resnet34 trained on classification data to get embeddings on faces, and used those embeddings to find how far away each picture pair was. I did not use center loss or other types of clustering loss functions (Due to time constraints), but expect my model would have performed much better for verificaiton if I did. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek1rL4Y9bxYI"
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pdb \n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from psutil import virtual_memory\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from scipy.spatial import distance \n",
        "#!pip install wandb --upgrade\n",
        "#import wandb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSOIgD73REpi"
      },
      "source": [
        "class SetUpColab():\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  #Determines how much ram the runtime has\n",
        "  @staticmethod\n",
        "  def runtime_info():\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "    if ram_gb < 20:\n",
        "      print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "      print('You are using a high-RAM runtime!')\n",
        "    !nvidia-smi\n",
        "    \n",
        "  @staticmethod\n",
        "  def mount_google_drive():\n",
        "    drive.mount('/content/gdrive')\n",
        "  \n",
        "  #Sets up environement for use with kaggle api\n",
        "  @staticmethod\n",
        "  def set_up_kaggle():\n",
        "    !pip uninstall -y kaggle\n",
        "    !pip install --upgrade pip\n",
        "    !pip install kaggle==1.5.6\n",
        "    !mkdir .kaggle\n",
        "\n",
        "    token = {\"username\":\"nicholasmagal\",\"key\":\"9bf671834d75b58fac2b037da15f4cf0\"}\n",
        "    with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "      json.dump(token, file)\n",
        "    \n",
        "    for i in range(2):\n",
        "      !chmod 600 /content/.kaggle/kaggle.json\n",
        "      !cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "      !kaggle config set -n path -v /content\n",
        "  \n",
        "  @staticmethod\n",
        "  def change_dir(path):\n",
        "    os.chdir(path)\n",
        "  \n",
        "  @staticmethod\n",
        "  def setup_wandb():\n",
        "    wandb.login()\n",
        "\n",
        "  \n",
        "  #Calls above methods to do a complete Collab setup, ready to run ml models :D Note may want to change this per competition\n",
        "  @staticmethod\n",
        "  def complete_set_up():\n",
        "    SetUpColab.runtime_info()\n",
        "    SetUpColab.mount_google_drive()\n",
        "    SetUpColab.set_up_kaggle()\n",
        "    #SetUpColab.setup_wandb()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_JPN35nD1Gp"
      },
      "source": [
        "SetUpColab.complete_set_up()\n",
        "data_url = 'idl-fall21-hw2p2s2-face-verification'\n",
        "data_path = '/content/competitions/' + data_url\n",
        "!kaggle competitions download -c idl-fall21-hw2p2s2-face-verification\n",
        "SetUpColab.change_dir(data_path)\n",
        "!unzip idl-fall21-hw2p2s2-face-verification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ-Mn3FSkbT_"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unl_yyRHkc5D"
      },
      "source": [
        "class ResidualBlockResnet34(nn.Module):\n",
        "  def __init__(self, input_channel_size, keep_dim = True, stride=1):\n",
        "    super().__init__()\n",
        "\n",
        "    #Depending if we are changing our dimensions, we will have to initlize our parameters differently\n",
        "    if keep_dim == True:  \n",
        "      output_size = input_channel_size\n",
        "    \n",
        "    else:\n",
        "      output_size = int(input_channel_size*2)\n",
        "\n",
        "    if stride > 1:\n",
        "      self.shortcut_x = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_channel_size, out_channels= output_size, kernel_size = 3, stride = stride, bias = False),\n",
        "      nn.BatchNorm2d(output_size))\n",
        "      self.conv0 = nn.Conv2d(in_channels=input_channel_size, out_channels = output_size, kernel_size=3,stride=stride, bias = False)\n",
        "    \n",
        "    else:\n",
        "      self.shortcut_x = nn.Identity()\n",
        "      self.conv0 = nn.Conv2d(in_channels=input_channel_size, out_channels = output_size, kernel_size=3,stride=stride, padding = 'same', bias = False)\n",
        "\n",
        "    self.bn_0 = nn.BatchNorm2d(output_size)\n",
        "    self.reLU_0 = nn.ReLU()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=output_size, out_channels = output_size, kernel_size = 3, stride = 1, padding='same', bias = False)\n",
        "    self.bn_1 = nn.BatchNorm2d(output_size)\n",
        "    self.reLU_1 = nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    #pdb.set_trace()\n",
        "    shortcut = self.shortcut_x(x)\n",
        "    \n",
        "    out = self.conv0(x)\n",
        "    out = self.bn_0(out)\n",
        "    out = self.reLU_0(out)\n",
        "    \n",
        "    out = self.conv1(out)\n",
        "    out = self.bn_1(out)\n",
        "    out = self.reLU_1(out)\n",
        "\n",
        "    out = out + shortcut\n",
        "\n",
        "    return(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90JVNrUakgvn"
      },
      "source": [
        "class Resnet34(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels,out_channels=64,kernel_size=3, stride =1, padding='same',bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(), \n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64, keep_dim = False, stride = 2),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        ResidualBlockResnet34(input_channel_size = 128, keep_dim = False, stride = 2),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256, keep_dim = False, stride = 2),\n",
        "        ResidualBlockResnet34(input_channel_size = 512),\n",
        "        ResidualBlockResnet34(input_channel_size = 512),\n",
        "        nn.AdaptiveAvgPool2d((1,1)),#add a dropout here to combat overfitting\n",
        "        nn.Flatten(),\n",
        "        nn.Dropout(p=0.2) \n",
        "    )\n",
        "\n",
        "    self.linear_layer = nn.Linear(512,4000)\n",
        "\n",
        "  \n",
        "  def forward(self, x, return_embedding=False):\n",
        "    x = self.cnn_layers(x)\n",
        "    embedding = x\n",
        "    x = self.linear_layer(x)\n",
        "\n",
        "    if return_embedding == True:\n",
        "      return(x,embedding)\n",
        "    return(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3jZUrAynQqB"
      },
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        feat_dim (int): feature dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        self.device = device\n",
        "        \n",
        "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\n",
        "\n",
        "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        "\n",
        "        dist = []\n",
        "        for i in range(batch_size):\n",
        "            value = distmat[i][mask[i]]\n",
        "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
        "            dist.append(value)\n",
        "        dist = torch.cat(dist)\n",
        "        loss = dist.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YI9j_EB18WU"
      },
      "source": [
        "class Resnet34_modified_closeness(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels,out_channels=64,kernel_size=3, stride =1, padding='same',bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(), \n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64),\n",
        "        ResidualBlockResnet34(input_channel_size = 64, keep_dim = False, stride = 2),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        ResidualBlockResnet34(input_channel_size = 128),\n",
        "        ResidualBlockResnet34(input_channel_size = 128, keep_dim = False, stride = 2),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256),\n",
        "        ResidualBlockResnet34(input_channel_size = 256, keep_dim = False, stride = 2),\n",
        "        ResidualBlockResnet34(input_channel_size = 512),\n",
        "        ResidualBlockResnet34(input_channel_size = 512),\n",
        "        nn.AdaptiveAvgPool2d((1,1)),#add a dropout here to combat overfitting\n",
        "        nn.Flatten(),\n",
        "        nn.Dropout(p=0.2) \n",
        "    )\n",
        "\n",
        "    self.linear_layer = nn.Linear(512,4000)\n",
        "    self.clossness_layer = nn.nn.Linear(512,4000) \n",
        "    self.close_ReLU = nn.ReL(inplace=True)\n",
        "\n",
        "  \n",
        "  def forward(self, x, return_embedding=False):\n",
        "    \n",
        "    output = self.cnn_layers(x)\n",
        "    embedding = output\n",
        "    output = self.linear_layer(output)\n",
        "\n",
        "    closeness_output = self.clossness_layer(x)\n",
        "    closenss_output = self.close_ReLU(closeness_output)\n",
        "\n",
        "    if return_embedding == True:\n",
        "      return(output,embedding)\n",
        "    return(output,closenss_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rur80Klg9O"
      },
      "source": [
        "class ModelComponents():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  @staticmethod\n",
        "  def load_model_for_inference(save_path, model):\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n",
        "\n",
        "  @staticmethod\n",
        "  def create_device():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device is on\",device)\n",
        "    return(device)\n",
        "    \n",
        "    def train(self, model, train_loader, dev_dataloader, optimizer, optimizer_close, epochs, loss_function, loss_function_close, path, device, length_val, scheduler):\n",
        "      for epoch in (range(epochs)):\n",
        "        avg_loss = 0.0\n",
        "\n",
        "        #training on data\n",
        "        model.train()\n",
        "        for batch_num, (feats, labels) in enumerate(train_loader):\n",
        "          feats, labels = feats.to(device), labels.to(device)\n",
        "          optimizer_close.zero_grad()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          output, clossness_output = model(feats)\n",
        "          loss = loss_function(output, labels.long())\n",
        "\n",
        "          c_loss = loss_function_close(clossness_output, labels.long() )\n",
        "\n",
        "          total_loss = loss + c_loss *1\n",
        "\n",
        "          total_loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          for param in loss_function_close.parameters():\n",
        "                param.grad.data *= (1. / 1)\n",
        "          \n",
        "          optimizer_closs.step()\n",
        "          \n",
        "          avg_loss += loss.item()\n",
        "\n",
        "          if batch_num % 10 == 9:\n",
        "              print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, avg_loss/50))\n",
        "              wandb.log({\"average loss\" : avg_loss/50, \"epoch\" : epoch})\n",
        "              avg_loss = 0.0 \n",
        "\n",
        "          #Clean Up\n",
        "          torch.cuda.empty_cache()\n",
        "          del feats\n",
        "          del labels\n",
        "        del loss\n",
        "      \n",
        "      #Running our model against validation examples\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        for batch_num, (x, y) in enumerate(dev_dataloader):\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          outputs = model(x)\n",
        "          num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "        \n",
        "        val_acc = num_correct / length_val\n",
        "        print('Epoch: {}, Validation Accuracy: {:.2f}'.format(epoch, val_acc))\n",
        "        wandb.log({\"Validation Accuracy\" : val_acc})\n",
        "      \n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        to_save_path = path + str(epoch) + '.pt'\n",
        "        self.save_model(model, optimizer, to_save_path, scheduler)\n",
        "  \n",
        "  def face_verification(self, comparison_list, model, device):\n",
        "\n",
        "    cosine_similarity = []\n",
        "    image_pair_list =[]\n",
        "\n",
        "    model.eval()\n",
        "    for image_pair in comparison_list:\n",
        "      #print(image_pair)\n",
        "      img0, img1 = self.return_image_pair(image_pair)\n",
        "      \n",
        "      img0 = img0.to(device)\n",
        "      img1 = img1.to(device)\n",
        "      \n",
        "\n",
        "      img0_embedding_logits, img0_embedding = model(img0, return_embedding=True)\n",
        "      img1_embedding_logits, img1_embedding = model(img1, return_embedding=True)\n",
        "\n",
        "      img0_embedding = img0_embedding.squeeze()\n",
        "      img1_embedding = img1_embedding.squeeze()\n",
        "\n",
        "      compute_sim = nn.CosineSimilarity(dim=0)\n",
        "      similarity = compute_sim(img0_embedding,img1_embedding).item()\n",
        "\n",
        "      image_pair_list.append(image_pair)\n",
        "      cosine_similarity.append(similarity)\n",
        "      \n",
        "    return(image_pair_list,cosine_similarity)\n",
        "\n",
        "  \n",
        "  def get_image_pair_list(self, pair_file_path):\n",
        "    #Read in file as a list of lines\n",
        "    with open(pair_file_path) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    #Now splitting each line to get the pairs seperated\n",
        "    for index, image_line in enumerate(lines):\n",
        "      lines[index] = image_line.split()\n",
        "\n",
        "    return(lines) \n",
        "\n",
        "  def return_image_pair(self, image_tuple_paths):\n",
        "    img1 = Image.open(image_tuple_paths[0])\n",
        "    img2 = Image.open(image_tuple_paths[1])\n",
        "\n",
        "    img1 = torchvision.transforms.ToTensor()(img1).unsqueeze(0)\n",
        "    img1 = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(img1)\n",
        "\n",
        "    img2 = torchvision.transforms.ToTensor()(img2).unsqueeze(0)\n",
        "    img2 = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(img2)\n",
        "\n",
        "    return img1, img2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jSShYN26DcC"
      },
      "source": [
        "#Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wj0qDdM6Cz6"
      },
      "source": [
        "numEpochs = 10\n",
        "num_feats = 3\n",
        "closs_weight = 1\n",
        "lr_cent = 0.5\n",
        "feat_dim = 10\n",
        "\n",
        "weightDecay = 5e-5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = Resnet34_modified_closeness(num_feats, hidden_sizes, num_classes, feat_dim)\n",
        "\n",
        "\n",
        "criterion_label = nn.CrossEntropyLoss()\n",
        "criterion_closs = CenterLoss(num_classes, feat_dim, device)\n",
        "optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agwu7JKneu79"
      },
      "source": [
        "#Evaluating Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HCNE0G4pBEX",
        "outputId": "dab504bb-82ef-43dc-961d-93fc817e2340"
      },
      "source": [
        "cd verification_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/competitions/idl-fall21-hw2p2s2-face-verification/verification_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXbA0lHtiBMG",
        "outputId": "494f6321-664d-4655-ad88-e5f316a91a8b"
      },
      "source": [
        "component_builder = ModelComponents()\n",
        "\n",
        "#Model Parts\n",
        "device = component_builder.create_device()\n",
        "inference_model = Resnet34(3)\n",
        "inference_model.to(device)\n",
        "model_path = '/content/gdrive/MyDrive/IDL/HW/HW2/P1/saves_run_1/23.pt'\n",
        "inference_model = component_builder.load_model_for_inference(model_path, inference_model)\n",
        "\n",
        "#Data parts\n",
        "test_data_set_path = '/content/competitions/idl-fall21-hw2p2s2-face-verification/verification_pairs_test.txt'\n",
        "test_image_pairs_list = component_builder.get_image_pair_list(test_data_set_path)\n",
        "\n",
        "#Running \n",
        "image_pairs, similarity= component_builder.face_verification(test_image_pairs_list, inference_model, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSPtbqBKezm6"
      },
      "source": [
        "#Now getting the similiarity inputted \n",
        "df = pd.DataFrame(similarity)\n",
        "df.columns = ['Category']\n",
        "\n",
        "#Now getting the index ready\n",
        "concat_image_pair = []\n",
        "for pair in image_pairs:\n",
        "  concat_image_pair.append(\" \".join(pair))\n",
        "\n",
        "df.insert(0,'Id',concat_image_pair) \n",
        "df=df.set_index('Id')\n",
        "\n",
        "df.to_csv('/content/gdrive/MyDrive/IDL/HW/HW2/P2/saves_run_0/result1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}