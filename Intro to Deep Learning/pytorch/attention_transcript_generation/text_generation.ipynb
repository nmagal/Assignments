{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "AejRmnNTeOjm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "import random\n",
        "import math\n",
        "drive.mount('/content/gdrive/')\n",
        "!pip install wandb --upgrade\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBGGhL7RDGd-",
        "outputId": "d873029e-b589-4ed0-fdf8-81ec8055d228"
      },
      "source": [
        "cd /content/gdrive/MyDrive/IDL/HW/HW4/P1/handout_5/hw4/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/IDL/HW/HW4/P1/handout_5/hw4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Tools"
      ],
      "metadata": {
        "id": "RKzFzoKveVT-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUcZAmXC70QV"
      },
      "source": [
        "def log_softmax(x, axis):\n",
        "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
        "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
        "    return ret - lsm\n",
        "\n",
        "\n",
        "def array_to_str(arr, vocab):\n",
        "    return \" \".join(vocab[a] for a in arr)\n",
        "\n",
        "\n",
        "def test_prediction(out, targ):\n",
        "    out = log_softmax(out, 1)\n",
        "    nlls = out[np.arange(out.shape[0]), targ]\n",
        "    nll = -np.mean(nlls)\n",
        "    return nll\n",
        "\n",
        "def test_generation(inp, pred, vocab):\n",
        "    outputs = u\"\"\n",
        "    for i in range(inp.shape[0]):\n",
        "        w1 = array_to_str(inp[i], vocab)\n",
        "        w2 = array_to_str(pred[i], vocab)\n",
        "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "source": [
        "# load all that we need\n",
        "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('../dataset/vocab.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "source": [
        "# data loader\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "\n",
        "    def __init__(self,seq_p, variance_seq, dataset, batch_size, shuffle=True, sequence_len = 10):       \n",
        "        self.data = np.concatenate(dataset, axis=0)\n",
        "        self.sequence_len = sequence_len\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seq_p = seq_p\n",
        "        self.variance_seq = variance_seq\n",
        "\n",
        "    def __iter__(self):\n",
        "        #We need to change our sequences every epoch\n",
        "        seq_draw = np.random.binomial(1,self.seq_p,1)\n",
        "        if seq_draw == 1:\n",
        "          self.sequence_len = self.sequence_len\n",
        "          self.sequence_len = abs(math.floor(np.random.normal(loc = self.sequence_len, scale = self.variance_seq )))+1\n",
        "        else:\n",
        "          self.sequence_len = self.sequence_len/2\n",
        "          self.sequence_len =abs(math.floor(np.random.normal(loc = self.sequence_len, scale = self.variance_seq )))+1\n",
        "\n",
        "        #Breaking the inputs and targets out \n",
        "        inputs = [self.data[index*self.sequence_len:self.sequence_len*index + self.sequence_len] for index in range(len(self.data)//self.sequence_len)]\n",
        "        targets = [self.data[index*self.sequence_len+1:self.sequence_len*index + self.sequence_len+1] for index in range(len(self.data)//self.sequence_len)]\n",
        "        assert len(inputs)==len(targets), \"inputs and targets are not the same length\"\n",
        "        assert len(inputs[0]) == len(targets[0]), \"inputs and targets are not the same length\"\n",
        "\n",
        "        #Creating a list of batches\n",
        "        inputs_batched = [inputs[index*self.batch_size:index*self.batch_size+self.batch_size] for index in range(len(inputs)//self.batch_size)]#Missing out on the end of the data, fix this or use small batch size\n",
        "        targets_batched = [targets[index*self.batch_size:index*self.batch_size+self.batch_size] for index in range(len(targets)//self.batch_size)]#Missing out on the end of the data, fix this or use small batch size\n",
        "        \n",
        "        #Converting the list of batches so that each batch is in the shape of Batch x Sequence length\n",
        "        for batch_index in range(len(inputs_batched)):\n",
        "          batch_refined = np.stack(inputs_batched[batch_index])\n",
        "          inputs_batched[batch_index] = batch_refined\n",
        "        for batch_index in range(len(targets_batched)):\n",
        "          batch_refined = np.stack(targets_batched[batch_index])\n",
        "          targets_batched[batch_index] = batch_refined\n",
        "\n",
        "        data = list(zip(inputs_batched, targets_batched))\n",
        "        \n",
        "        if self.shuffle == True:\n",
        "          random.shuffle(data)\n",
        "        \n",
        "        for inputs, targets in data:\n",
        "          yield torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, e_size = 400, h_l_size = 1150, num_layers = 3, dropout = .2, bidirectional = False, sequence_len = 10):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings = 33278, embedding_dim = e_size)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        #LSTM model takes in inputs as N X L X H \n",
        "        self.LSTM = nn.LSTM(input_size = e_size, hidden_size = h_l_size, num_layers = num_layers, dropout = dropout, bidirectional = bidirectional, batch_first = True)\n",
        "        self.linear_layers = nn.Linear(1150, 33278, bias=True)\n",
        "\n",
        "        #apply weight tying \n",
        "        self.linear_layers.weight = self.embedding.weight\n",
        "\n",
        "        self.softmax = nn.LogSoftmax(dim = 2)\n",
        "\n",
        "    def forward(self, x, prediction = False):\n",
        "        #Embedding transforms data from B X L --> B X L X Input size\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x, _ = self.LSTM(x)\n",
        "        x = self.linear_layers(x)\n",
        "\n",
        "        #Don't pass through \n",
        "        if prediction == False:\n",
        "          x = self.softmax(x)\n",
        "        \n",
        "        return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XynDynLFNQr-"
      },
      "source": [
        "class LangaugeModelPrecise(nn.Module):\n",
        "\n",
        "  def __init__(self, linear_layer_h_0, e_size = 400, h_0_size = 1150, dropout = .5, vocab_size = 33278):\n",
        "    super(LangaugeModelPrecise, self).__init__()\n",
        "    \n",
        "    self.vocab_size = vocab_size\n",
        "    self.e_size = e_size\n",
        "    self.h_0_size = h_0_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(num_embeddings = 33278, embedding_dim = e_size)\n",
        "    self.lstm0 = nn.LSTMCell(input_size=e_size, hidden_size = h_0_size)\n",
        "    self.lstm1 = nn.LSTMCell(input_size=h_0_size, hidden_size = h_0_size)\n",
        "    self.lstm2 = nn.LSTMCell(input_size=h_0_size, hidden_size = e_size)\n",
        "\n",
        "    self.bn_0 = nn.BatchNorm1d(num_features = e_size)\n",
        "    self.bn_1 = nn.BatchNorm1d(num_features = linear_layer_h_0)\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "    self.linear_layers = nn.Sequential(self.bn_0, nn.ReLU(),self.dropout, nn.Linear(e_size, linear_layer_h_0, bias=False),self.bn_1, nn.ReLU(), torch.nn.Dropout(), nn.Linear(linear_layer_h_0,33278))\n",
        "    self.softmax = nn.LogSoftmax(dim = 2)\n",
        "\n",
        "  def forward(self, x, prediction = False):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    #Output = B x time X size ---> time X B X size\n",
        "    x = torch.permute(x, (1,0,2))\n",
        "    time_len, batch_size, input_after_embedding_size = x.shape\n",
        "\n",
        "    #init states\n",
        "    lstm0_states = (torch.zeros(batch_size, self.h_0_size), torch.zeros(batch_size, self.h_0_size))\n",
        "    lstm1_states = (torch.zeros(batch_size, self.h_0_size), torch.zeros(batch_size, self.h_0_size))\n",
        "    lstm2_states = (torch.zeros(batch_size, self.e_size), torch.zeros(batch_size, self.e_size))\n",
        "\n",
        "    #output tensor\n",
        "    stored_output = torch.empty((time_len, batch_size, self.vocab_size))\n",
        "\n",
        "    #Looping through time \n",
        "    for time_step in range(x.shape[0]):\n",
        "\n",
        "      #Getting through lstm layers\n",
        "      lstm0_states = self.lstm0(x[time_step],lstm0_states)\n",
        "      h_0, m_0 = lstm0_states\n",
        "      h_0 = self.activation(h_0)\n",
        "      h_0 = self.dropout(h_0)\n",
        "      \n",
        "      lstm1_states = self.lstm1(h_0,lstm1_states)\n",
        "      h_1, m_1 = lstm1_states\n",
        "      h_1 = self.activation(h_1)\n",
        "      h_1 = self.dropout(h_1)\n",
        "\n",
        "      lstm2_states = self.lstm2(h_1, lstm2_states)\n",
        "      h_2, m_2 = lstm2_states\n",
        "\n",
        "      #Linear layers \n",
        "      output_one_time_step = self.linear_layers(h_2)\n",
        "\n",
        "      stored_output[time_step] = output_one_time_step\n",
        "    if prediction == False:\n",
        "      stored_output = self.softmax(stored_output)\n",
        "    \n",
        "    return(stored_output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training/Testing"
      ],
      "metadata": {
        "id": "InWHTz9eekxC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "source": [
        "# TODO: define other hyperparameters here\n",
        "config = {\n",
        "    'epochs' : 50,\n",
        "    'lr' : .1,\n",
        "    'optimizer' : 'adam',\n",
        "    'batch_size' : 128,\n",
        "    'schedular' : 'ReduceLROnPlateau',\n",
        "    'weight_decay' : 5e-6,\n",
        "    'LSTM_hidden' : 1150,\n",
        "    'LSTM_layers' : 3,\n",
        "    'dropout': .25,\n",
        "    'patience' : 2,\n",
        "    'factor' : .2,\n",
        "    'embedding' : 'wordembedding',\n",
        "    'search_type' : 'greedy',\n",
        "    'random_batch' : \"yes\",\n",
        "    'dynamic sequence' : \"yes\",\n",
        "    'locked dropout' : \"None\",\n",
        "    \"Emedding dropout\": \"None\",\n",
        "    'num_layers': 3,\n",
        "    'dropout_lstm': .2,\n",
        "    'bidirectional': False,\n",
        "    'seq_len': 50,\n",
        "    'seq_p': .8,\n",
        "    'variance_seq': 10,\n",
        "    'linear_layer_h_0' : 600,\n",
        "    'e_size' : 1150,\n",
        "    'h_0_size' : 1150\n",
        "}\n",
        "\n",
        "#USE ADAM or ASGD increase embedding size and "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        \n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(),lr =config['lr'] , weight_decay=config['weight_decay'])\n",
        "        self.criterion = nn.NLLLoss()\n",
        "        self.schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=config['patience'], factor=config['factor'], verbose = True)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "        \n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "        wandb.log({\"Training Average Loss\":epoch_loss, \"epoch\":self.epochs })\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        #Inputs are B x Sequence Length\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "        output = self.model(inputs)\n",
        "\n",
        "        #FOR LSTM Output is given in dim B X L X C, must change to B X C X L \n",
        "        output = output.permute(0,2,1)\n",
        "\n",
        "        #FOR LSTMCELL time X B X size --> B X C X L\n",
        "        #output = output.permute(1, 2, 0)\n",
        "\n",
        "        loss = self.criterion(output, targets.long())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return(loss)\n",
        "\n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        self.schedular.step(nll)\n",
        "        \n",
        "        wandb.log({\"Testing Average Loss\":nll})\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        inp = torch.from_numpy(inp)\n",
        "        inp = inp.to(trainer.device)\n",
        "        predictions = model(inp, prediction = True)\n",
        "        # Predictions is in the form of B X L X C\n",
        "        predicted_word = predictions[:,-1,:]\n",
        "        return(predicted_word.cpu().detach().numpy())\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"\n",
        "        #inp shape is B x Seq_len\n",
        "        inp = torch.from_numpy(inp)\n",
        "        inp = inp.to(trainer.device)\n",
        "        output = model(inp)\n",
        "        batch_size, L, C = output.shape\n",
        "\n",
        "        #B X L X C is our output dimensions\n",
        "        predicted_words = torch.argmax(output, dim = 2)\n",
        "        predicted_words = predicted_words[:,-1]\n",
        "        predicted_words = torch.reshape(predicted_words,(-1,1))\n",
        "        #Storing out generated words\n",
        "        full_forward_predictions = []\n",
        "        full_forward_predictions.append(predicted_words.cpu().detach())\n",
        "\n",
        "        #Generating forward words\n",
        "        for word in range(forward-1):\n",
        "          output = model(predicted_words)\n",
        "          #Getting our prediction and storing it\n",
        "          predicted_words = torch.argmax(output, dim = 2)\n",
        "          full_forward_predictions.append(predicted_words.cpu().detach())\n",
        "\n",
        "        full_forward_predictions = torch.cat(full_forward_predictions, dim = 1)\n",
        "        return full_forward_predictions.cpu().detach().numpy()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCVG5YISwRW",
        "outputId": "31249f60-dce1-4997-cbc6-81c7949791d8"
      },
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1637808207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "source": [
        "model = LanguageModel(e_size = config['e_size'],h_l_size = config['LSTM_hidden'],num_layers = config['num_layers'], dropout = config['dropout_lstm'], bidirectional = config['bidirectional'] )\n",
        "#model = LangaugeModelPrecise(linear_layer_h_0=config[\"linear_layer_h_0\"], e_size = config['e_size'], h_0_size=config['h_0_size'],dropout=config['dropout'])\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=config['batch_size'], shuffle=True,sequence_len = config[\"seq_len\"], seq_p =config[\"seq_p\"], variance_seq =config['variance_seq'])\n",
        "\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=config['epochs'], run_id=run_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D8wTJkBSwRc"
      },
      "source": [
        "best_nll = 1e30 \n",
        "wandb.init(project=\"Language Model\", config=config)\n",
        "for epoch in range(config['epochs']):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2FmDqBCSwRf"
      },
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipdbmqaGSwRh"
      },
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}