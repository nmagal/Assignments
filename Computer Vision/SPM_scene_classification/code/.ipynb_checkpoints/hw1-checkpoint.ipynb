{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "from copy import copy\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage\n",
    "import skimage.color\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-720 Computer Vision: Homework 1 (Spring 2022)\n",
    "## Spatial Pyramid Matching for Scene Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir=\"../data\",\n",
    "        feat_dir=\"../feat\",\n",
    "        out_dir=\".\",\n",
    "        filter_scales=(1, 2),\n",
    "        K=10,\n",
    "        alpha=25,\n",
    "        L=1,\n",
    "    ):\n",
    "        '''\n",
    "        Manage tunable hyperparameters.\n",
    "\n",
    "        You can also add your own additional hyperparameters.\n",
    "\n",
    "        [input]\n",
    "        * data_dir: Data directory.\n",
    "        * feat_dir: Feature directory.\n",
    "        * out_dir: Output directory.\n",
    "        * filter_scales: A list of scales for all the filters.\n",
    "        * K: Number of words.\n",
    "        * alpha: Subset of alpha pixels in each image.\n",
    "        * L: Number of layers in spatial pyramid matching (SPM).\n",
    "\n",
    "        '''\n",
    "        self.data_dir = data_dir\n",
    "        self.feat_dir = feat_dir\n",
    "        self.out_dir = out_dir\n",
    "        self.filter_scales = list(filter_scales)\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.L = L\n",
    "\n",
    "opts = Opts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def get_num_CPU():\n",
    "    '''\n",
    "    Counts the number of CPUs available in the machine.\n",
    "    '''\n",
    "    return multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "def display_filter_responses(opts, response_maps):\n",
    "    '''\n",
    "    Visualizes the filter response maps.\n",
    "\n",
    "    [input]\n",
    "    * response_maps: a numpy.ndarray of shape (H,W,3F)\n",
    "    '''\n",
    "\n",
    "    n_scale = len(opts.filter_scales)\n",
    "    plt.figure()\n",
    "\n",
    "    for i in range(n_scale * 4):\n",
    "        plt.subplot(n_scale, 4, i + 1)\n",
    "        resp = response_maps[:, :, i * 3:i * 3 + 3]\n",
    "        resp_min = resp.min(axis=(0, 1), keepdims=True)\n",
    "        resp_max = resp.max(axis=(0, 1), keepdims=True)\n",
    "        resp = (resp - resp_min) / (resp_max - resp_min)\n",
    "        plt.imshow(resp)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, top=0.95,\n",
    "                        bottom=0.05, wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_wordmap(original_image, wordmap, out_path=None):\n",
    "    fig = plt.figure(figsize=(12.8, 4.8))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.imshow(original_image)\n",
    "    plt.axis(\"off\")\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.imshow(wordmap)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "## Q1.1.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filter_responses(opts, img):\n",
    "    '''\n",
    "    Extracts the filter responses for the given image.\n",
    "\n",
    "    [input]\n",
    "    * opts    : options\n",
    "    * img    : numpy.ndarray of shape (H,W) or (H,W,3)\n",
    "    [output]\n",
    "    * filter_responses: numpy.ndarray of shape (H,W,3F)\n",
    "    '''\n",
    "\n",
    "    filter_scales = opts.filter_scales\n",
    "    # ----- TODO -----\n",
    "\n",
    "    filter_responses = None\n",
    "\n",
    "    return filter_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6d/_fq6qgb54_gbt5w6f1246nd80000gn/T/ipykernel_6753/3513535581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilter_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_filter_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdisplay_filter_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_responses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/6d/_fq6qgb54_gbt5w6f1246nd80000gn/T/ipykernel_6753/766635523.py\u001b[0m in \u001b[0;36mdisplay_filter_responses\u001b[0;34m(opts, response_maps)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mresp_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresp_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHYAAABSCAYAAACBmiAxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD3klEQVR4nO2dwWtcVRSHv5+13WTTRQKKiigEQ1wIdSh1I9kU2lBw46LdCG6Con+AK/0fFLFkUcSNLkuRFrd1o3QibamCEAUxKJgqVIqiFI6LecoQJpmbl/syz8Pvgwczc997c7gf72VyOPc8RQQmHw/NOgDTDRabFItNisUmxWKTYrFJmSpW0iVJv0i6s8u4JL0raVPSbUkn6odp9kvJFfshcGaP8bPAYrOtAR8cPCxzUKaKjYjrwG977PIS8FGM+AI4LunRWgGadtT4G/sY8OPY+63mMzNDHq5wDk34bGKeUtIao9s1c3Nzzy8tLVX4+rxsbGzcjYiFNsfWELsFPDH2/nHgp0k7RsQ6sA4wGAxiOBxW+Pq8SPqh7bE1bsVXgFeaX8engHsR8XOF85oDMPWKlfQxsALMS9oC3gGOAkTEReAqsApsAn8Ar3YVrClnqtiIuDBlPIA3qkVkquDMU1IsNikWmxSLTYrFJsVik2KxSbHYpFhsUiw2KRabFItNisUmxWKTYrFJKRIr6Yykb5va4bcmjK9IuifpZrO9XT9Usx9KKiiOAO8DpxnVN92QdCUivtmx6+cRca6DGE0LSq7Yk8BmRHwfEX8DnzCqJTY9pkRsad3wC5JuSbom6dlJJ5K0Jmkoabi9vd0iXFNKidiSuuGvgCcj4jngPeDypBNFxHpEDCJisLDQqlzWFFIidmrdcET8HhH3m9dXgaOS5qtFafZNidgbwKKkpyQdA84zqiX+D0mPSFLz+mRz3l9rB2vKKSk/fSDpTeAz4AhwKSK+lvRaM34ReBl4XdID4E/gfLgdzUzRrObfSzymI2kjIgZtjnXmKSkWmxSLTYrFJsVik2KxSbHYpFhsUiw2KRabFItNisUmxWKTYrFJqVV+6ta2PaOkX/G/5adngWXggqTlHbu5tW3PqFV+6ta2PaNW+alb2/aMku6nJeWnRa1tx9vaAn/t1o5+hswDd2cdxBjPtD2wRGxJ29qi1rbjbW0lDdvW83RF32KS1LoorEr5KW5t2ztqlZ+6tW3PmFn5qaS15tbcG/oW00HimZlY0y1OKSalc7F9S0f2bXV+Z08ii4jONkY/tr4DngaOAbeA5R37rALXGP0vfAr4csbxrACfdjkvO77vReAEcGeX8Vbz0/UV27d0ZO9W50dHTyLrWmzf0pHVVucfIq3mp8YDlfaiWjqyEvtZnX9f0iqj1fmLHcVTQqv56fqKrZaOPKx4on+r81vNT9di+5aO/D+uzm81P53eivuWjiyM51BX56ujJ5E585QUZ56SYrFJsdikWGxSLDYpFpsUi02KxSblHxELyqBOoU0sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Should have filters for at least 3 scales.\n",
    "\n",
    "opts.filter_scales = [1, 2, 4, 8]\n",
    "img_path = join(opts.data_dir, 'kitchen/sun_aasmevtpkslccptd.jpg')\n",
    "img = plt.imread(img_path) / 255.\n",
    "filter_responses = extract_filter_responses(opts, img)\n",
    "display_filter_responses(opts, filter_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dictionary_one_image(args):\n",
    "    \"\"\"\n",
    "    Extracts a random subset of filter responses of an image and save it to disk\n",
    "    This is a worker function called by compute_dictionary\n",
    "\n",
    "    Your are free to make your own interface based on how you implement compute_dictionary\n",
    "    \"\"\"\n",
    "    opts, idx, img_path = args\n",
    "    # ----- TODO -----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_dictionary(opts, n_worker=1):\n",
    "    \"\"\"\n",
    "    Creates the dictionary of visual words by clustering using k-means.\n",
    "\n",
    "    [input]\n",
    "    * opts         : options\n",
    "    * n_worker     : number of workers to process in parallel\n",
    "\n",
    "    [saved]\n",
    "    * dictionary : numpy.ndarray of shape (K,3F)\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = opts.data_dir\n",
    "    feat_dir = opts.feat_dir\n",
    "    out_dir = opts.out_dir\n",
    "    K = opts.K\n",
    "\n",
    "    train_files = open(join(data_dir, \"train_files.txt\")).read().splitlines()\n",
    "    # ----- TODO -----\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = get_num_CPU()\n",
    "compute_dictionary(opts, n_worker=n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3\n",
    "\n",
    "The wordmap shows the contours in each image. Words change along the edges and tend to stay the same for homogenous regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_words(opts, img, dictionary):\n",
    "    \"\"\"\n",
    "    Compute visual words mapping for the given img using the dictionary of visual words.\n",
    "\n",
    "    [input]\n",
    "    * opts    : options\n",
    "    * img    : numpy.ndarray of shape (H,W) or (H,W,3)\n",
    "\n",
    "    [output]\n",
    "    * wordmap: numpy.ndarray of shape (H,W)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- TODO -----\n",
    "    wordmap = None\n",
    "\n",
    "    return wordmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = np.load(join(opts.out_dir, 'dictionary.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = join(opts.data_dir, 'kitchen/sun_aasmevtpkslccptd.jpg')\n",
    "img = plt.imread(img_path) / 255.\n",
    "wordmap = get_visual_words(opts, img, dictionary)\n",
    "visualize_wordmap(img, wordmap)\n",
    "\n",
    "img_path = join(opts.data_dir, 'highway/sun_ailjxpgyepocjdos.jpg')\n",
    "img = plt.imread(img_path) / 255.\n",
    "wordmap = get_visual_words(opts, img, dictionary)\n",
    "visualize_wordmap(img, wordmap)\n",
    "\n",
    "img_path = join(opts.data_dir, 'laundromat/sun_aabvooxzwmzzvwds.jpg')\n",
    "img = plt.imread(img_path) / 255.\n",
    "wordmap = get_visual_words(opts, img, dictionary)\n",
    "visualize_wordmap(img, wordmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_wordmap(opts, wordmap):\n",
    "    '''\n",
    "    Compute histogram of visual words.\n",
    "\n",
    "    [input]\n",
    "    * opts      : options\n",
    "    * wordmap   : numpy.ndarray of shape (H,W)\n",
    "\n",
    "    [output]\n",
    "    * hist: numpy.ndarray of shape (K)\n",
    "    '''\n",
    "\n",
    "    K = opts.K\n",
    "    # ----- TODO -----\n",
    "    hist = None\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_from_wordmap(opts, wordmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_wordmap_SPM(opts, wordmap):\n",
    "    \"\"\"\n",
    "    Compute histogram of visual words using spatial pyramid matching.\n",
    "\n",
    "    [input]\n",
    "    * opts      : options\n",
    "    * wordmap   : numpy.ndarray of shape (H,W)\n",
    "\n",
    "    [output]\n",
    "    * hist_all: numpy.ndarray of shape (K*(4^L-1)/3)\n",
    "    \"\"\"\n",
    "\n",
    "    K = opts.K\n",
    "    L = opts.L\n",
    "    # ----- TODO -----\n",
    "    hist_all = None \n",
    "\n",
    "    return hist_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_set(word_hist, histograms):\n",
    "    \"\"\"\n",
    "    Compute the distance between a histogram of visual words with all training image histograms.\n",
    "\n",
    "    [input]\n",
    "    * word_hist: numpy.ndarray of shape (K)\n",
    "    * histograms: numpy.ndarray of shape (N,K)\n",
    "\n",
    "    [output]\n",
    "    * dists: numpy.ndarray of shape (N)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- TODO -----\n",
    "    dists = None\n",
    "    \n",
    "    return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature(opts, img_path, dictionary):\n",
    "    \"\"\"\n",
    "    Extracts the spatial pyramid matching feature.\n",
    "\n",
    "    [input]\n",
    "    * opts      : options\n",
    "    * img_path  : path of image file to read\n",
    "    * dictionary: numpy.ndarray of shape (K, 3F)\n",
    "\n",
    "\n",
    "    [output]\n",
    "    * feature: numpy.ndarray of shape (K)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- TODO -----\n",
    "    feature = None\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recognition_system(opts, n_worker=1):\n",
    "    \"\"\"\n",
    "    Creates a trained recognition system by generating training features from all training images.\n",
    "\n",
    "    [input]\n",
    "    * opts        : options\n",
    "    * n_worker  : number of workers to process in parallel\n",
    "\n",
    "    [saved]\n",
    "    * features: numpy.ndarray of shape (N,M)\n",
    "    * labels: numpy.ndarray of shape (N)\n",
    "    * dictionary: numpy.ndarray of shape (K,3F)\n",
    "    * SPM_layer_num: number of spatial pyramid layers\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = opts.data_dir\n",
    "    out_dir = opts.out_dir\n",
    "    SPM_layer_num = opts.L\n",
    "\n",
    "    train_files = open(join(data_dir, \"train_files.txt\")).read().splitlines()\n",
    "    train_labels = np.loadtxt(join(data_dir, \"train_labels.txt\"), np.int32)\n",
    "    dictionary = np.load(join(out_dir, \"dictionary.npy\"))\n",
    "\n",
    "    # ----- TODO -----\n",
    "\n",
    "    # example code snippet to save the learned system\n",
    "    # np.savez_compressed(join(out_dir, 'trained_system.npz'),\n",
    "    #     features=features,\n",
    "    #     labels=train_labels,\n",
    "    #     dictionary=dictionary,\n",
    "    #     SPM_layer_num=SPM_layer_num,\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_recognition_system(opts, n_worker=n_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recognition_system(opts, n_worker=1):\n",
    "    \"\"\"\n",
    "    Evaluates the recognition system for all test images and returns the confusion matrix.\n",
    "\n",
    "    [input]\n",
    "    * opts        : options\n",
    "    * n_worker  : number of workers to process in parallel\n",
    "\n",
    "    [output]\n",
    "    * conf: numpy.ndarray of shape (8,8)\n",
    "    * accuracy: accuracy of the evaluated system\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = opts.data_dir\n",
    "    out_dir = opts.out_dir\n",
    "\n",
    "    trained_system = np.load(join(out_dir, \"trained_system.npz\"))\n",
    "    dictionary = trained_system[\"dictionary\"]\n",
    "\n",
    "    # using the stored options in the trained system instead of opts.py\n",
    "    test_opts = copy(opts)\n",
    "    test_opts.K = dictionary.shape[0]\n",
    "    test_opts.L = trained_system[\"SPM_layer_num\"]\n",
    "\n",
    "    test_files = open(join(data_dir, \"test_files.txt\")).read().splitlines()\n",
    "    test_labels = np.loadtxt(join(data_dir, \"test_labels.txt\"), np.int32)\n",
    "\n",
    "    # ----- TODO -----\n",
    "    conf, accuracy = None, None\n",
    "\n",
    "    return conf, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf, accuracy = evaluate_recognition_system(opts, n_worker=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "classes = [\n",
    "    \"aquarium\", \"desert\", \"highway\", \"kitchen\",\n",
    "    \"laundromat\", \"park\", \"waterfall\", \"windmill\",\n",
    "]\n",
    "df = pd.DataFrame(conf, columns=classes)\n",
    "df.insert(0, \"\", classes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.3 (Extra Credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IDF(opts, n_worker=1):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def evaluate_recognition_System_IDF(opts, n_worker=1):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
